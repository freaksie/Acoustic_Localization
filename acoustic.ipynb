{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 17:22:14.435847: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-29 17:22:14.583358: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-29 17:22:14.612440: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-29 17:22:15.097870: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/neel/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.8/lib64:\n",
      "2022-12-29 17:22:15.097933: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/neel/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.8/lib64:\n",
      "2022-12-29 17:22:15.097939: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.optimizers import Adam, Adamax, SGD\n",
    "import imutils\n",
    "import tensorflow_addons as tfa\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "gpu = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "print(gpu)\n",
    "tf.keras.backend.clear_session()\n",
    "TF_ENABLE_ONEDNN_OPTS=0\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(tf.device('/gpu:0'))\n",
    "\n",
    "tf.device('/gpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"/home/neel/Acoustic/Acoustics/data\"\n",
    "spectrogram_path=os.path.sep.join([base_path,\"images\"])\n",
    "annots_path=os.path.sep.join([base_path,\"train.csv\"])\n",
    "\n",
    "base_output=\"/home/neel/Acoustic/Acoustics/output_mit\"\n",
    "model_path=os.path.sep.join([base_output,\"detector.h5\"])\n",
    "plot_path=os.path.sep.join([base_output,\"plot.png\"])\n",
    "test_file=os.path.sep.join([base_output,\"test.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image1 = cv2.imread(\"/home/neel/Acoustic/yolov5_training/yolov5/runs/detect/frames_test/3m_train0_1.jpg\")\n",
    "# (h, w) = image1.shape[:2]\n",
    "# print(h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] loading dataset...\")\n",
    "rows = open(annots_path).read().strip().split(\"\\n\")\n",
    "spectrogram=np.empty(((len(rows)),216,216,6), dtype=\"float32\")\n",
    "bounding_box_cords=[]\n",
    "filenames=[]\n",
    "cnt=0\n",
    "for row in rows:\n",
    "    row = row.split(\",\")\n",
    "    (filename1,filename2,X, Y, W, H) = row\n",
    "    imagePath1 = os.path.sep.join([spectrogram_path, filename1])\n",
    "    imagePath2 = os.path.sep.join([spectrogram_path, filename2])\n",
    "    # image1 = cv2.imread(imagePath1)\n",
    "    # image2=cv2.imread(imagePath2)\n",
    "    # (h, w) = image1.shape[:2]\n",
    "\n",
    "    # startX = float(startX) / w\n",
    "    # startY = float(startY) / h\n",
    "    # endX = float(endX) / w\n",
    "    # endY = float(endY) / h\n",
    "\n",
    "    image1 = tf.io.read_file(imagePath1)\n",
    "    image1=tf.image.decode_image(image1,channels=3,dtype=tf.float32) \n",
    "    image1=tf.image.resize(image1,[216,216])\n",
    "    image2 = tf.io.read_file(imagePath2)\n",
    "    image2=tf.image.decode_image(image2,channels=3,dtype=tf.float32) \n",
    "    image2=tf.image.resize(image2,[216,216])\n",
    "    # print(image1.shape)\n",
    "    data=np.concatenate((image1,image2),axis=-1)\n",
    "    spectrogram[cnt]=data / 255.0\n",
    "    cnt+=1\n",
    "    bounding_box_cords.append((X, Y, W, H))\n",
    "    filenames.append([filename1,filename2])\n",
    "    # filenames.append(filename1)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = np.array(spectrogram1, dtype=\"float32\") / 255.0    #(example , height, width, rgb*channel) = (316,640,640,3*2)\n",
    "# data2 = np.array(spectrogram2, dtype=\"float32\") / 255.0 \n",
    "# targets = np.array(bounding_box_cords, dtype=\"float32\")\n",
    "# # data1.shape   #(example, cords) = (316,4)\n",
    "# data=np.concatenate((data1,data2),axis=-1)\n",
    "# data.shape\n",
    "targets = np.array(bounding_box_cords, dtype=\"float32\")\n",
    "print(targets.shape)\n",
    "del bounding_box_cords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = train_test_split(spectrogram, targets, filenames, test_size=0.10,random_state=42)\n",
    "del spectrogram, targets,filenames\n",
    "(trainImages, testImages) = split[:2]\n",
    "(trainTargets, testTargets) = split[2:4]\n",
    "(trainFilenames, testFilenames) = split[4:]\n",
    "\n",
    "print(\"[INFO] saving testing filenames...\")\n",
    "f = open(test_file, \"w\")\n",
    "for i in testFilenames:\n",
    "    f.write(i[0]+\",\"+i[1])\n",
    "    f.write(\"\\n\")\n",
    "# f.write(\"\\n\".join(testFilenames))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainTargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 17:22:17.949715: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-29 17:22:19.688301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 210 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:19:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model= tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32,(3,3), padding='valid',strides=2, activation=\"relu\", input_shape=(216,216,6)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32,(3,3), padding='valid',strides=2, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(64,(3,3), padding='valid', strides=2, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='valid'))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(64,(3,3), padding='same', activation=\"relu\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(128,(3,3), padding='same',strides=2, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# model.add(tf.keras.layers.Conv2D(256,(3,3), padding='same', strides=2, activation=\"relu\"))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# model.add(tf.keras.layers.Conv2D(512,(3,3), padding='valid', strides=2, activation=\"relu\"))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# model.add(tf.keras.layers.Conv2D(512,(3,3), padding='valid',strides=2, activation=\"relu\"))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# model.add(tf.keras.layers.Conv2D(1024,(3,3), padding='valid', activation=\"relu\"))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# model.add(tf.keras.layers.Conv2D(1024,(3,3), padding='same', activation=\"relu\"))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "# model.add(tf.keras.layers.Conv2DTranspose(1024,(3,3), padding='valid', activation=\"relu\"))\n",
    "# model.add(tf.keras.layers.Conv2DTranspose(1024,(3,3), padding='valid', activation=\"relu\"))\n",
    "# model.add(tf.keras.layers.Conv2DTranspose(1024,(3,3), padding='valid',strides=2, activation=\"relu\"))\n",
    "# model.add(tf.keras.layers.Conv2DTranspose(1024,(3,3), padding='valid', activation=\"relu\"))\n",
    "\n",
    "# model.add(tf.keras.layers.Conv2D(512,(3,3), padding='same', activation=\"relu\"))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.Conv2D(256,(3,3), padding='same', activation=\"relu\"))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.Conv2D(125,(3,3), padding='same', activation=\"relu\"))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(4, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 107, 107, 32)      1760      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 107, 107, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 53, 53, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 26, 26, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 12, 12, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 6, 6, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 3, 3, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1152)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               147584    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 299,620\n",
      "Trainable params: 298,980\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "init_lr=1e-4\n",
    "\n",
    "epoch=60\n",
    "batch_size=32\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    init_lr,\n",
    "    decay_steps=20,\n",
    "    decay_rate=0.1,\n",
    "    staircase=True)\n",
    "\n",
    "opt = SGD(learning_rate=lr_schedule ,momentum=0.9, )\n",
    "model.compile(\n",
    "  optimizer='sgd',\n",
    "  loss='mse',\n",
    "  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "\tH = model.fit(\n",
    "\t\ttrainImages, trainTargets,\n",
    "\t\tvalidation_data=(testImages, testTargets),\n",
    "\t\tbatch_size=batch_size,\n",
    "\t\tepochs=epoch,\n",
    "\t\tverbose=1)\n",
    "\tprint(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"[INFO] saving object detector model...\")\n",
    "model.save(model_path, save_format=\"h5\")\n",
    "N = epoch\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_loss\")\n",
    "plt.title(\"Bounding Box Regression MSE on Training Set\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=load_model(model_path,custom_objects={'custom_mse': custom_mse})\n",
    "model=load_model(model_path)\n",
    "for img in trainFilenames:\n",
    "    imagePath1=os.path.sep.join([spectrogram_path, img[0]])\n",
    "    imagePath2=os.path.sep.join([spectrogram_path, img[1]])\n",
    "    image1 = tf.io.read_file(imagePath1)\n",
    "    image1=tf.image.decode_image(image1,channels=3,dtype=tf.float32) \n",
    "    image1=tf.image.resize(image1,[216,216])\n",
    "    image2 = tf.io.read_file(imagePath2)\n",
    "    image2=tf.image.decode_image(image2,channels=3,dtype=tf.float32) \n",
    "    image2=tf.image.resize(image2,[216,216])\n",
    "    image1 = np.expand_dims(image1, axis=0)\n",
    "    image2 = np.expand_dims(image2, axis=0)\n",
    "    image=np.concatenate((image1,image2),axis=-1)\n",
    "\n",
    "    preds = model.predict(image)[0]\n",
    "    (startX, startY, endX, endY) = preds\n",
    "    # image2 = cv2.imread(os.path.sep.join([\"/home/neel/Acoustic/yolov5_training/img_data/frames\", img[0][9:]]))\n",
    "    # image2 = imutils.resize(image2, width=600)\n",
    "    # (h, w) = image1.shape[:2]\n",
    "    # startX = startX*w\n",
    "    # startY = startY*h\n",
    "    # endX = endX*w\n",
    "    # endY = endY*h\n",
    "    f=open(\"/home/neel/Acoustic/Acoustics/data/labels/\"+img[0][9:-4]+\".txt\")\n",
    "    ground_truth=f.read()\n",
    "    print(\"Ground truth: \",ground_truth[1:])\n",
    "    print(\"Predicted: \",startX, startY, endX, endY)\n",
    "    # cv2.rectangle(image2, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "\t# # show the output image\n",
    "    # cv2.imwrite(\"/home/neel/Acoustic/Acoustics/output/test_img/\"+img[0], image2)\n",
    "    # cv2.imshow(\"Output\", image2)\n",
    "    # cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(box1, box2, xywh=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n",
    "    # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n",
    "\n",
    "    # Get the coordinates of bounding boxes\n",
    "    if xywh:  # transform from xywh to xyxy\n",
    "        (x1, y1, w1, h1), (x2, y2, w2, h2) = box1.chunk(4, 1), box2.chunk(4, 1)\n",
    "        w1_, h1_, w2_, h2_ = w1 / 2, h1 / 2, w2 / 2, h2 / 2\n",
    "        b1_x1, b1_x2, b1_y1, b1_y2 = x1 - w1_, x1 + w1_, y1 - h1_, y1 + h1_\n",
    "        b2_x1, b2_x2, b2_y1, b2_y2 = x2 - w2_, x2 + w2_, y2 - h2_, y2 + h2_\n",
    "    else:  # x1, y1, x2, y2 = box1\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, 1)\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, 1)\n",
    "        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1\n",
    "        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1\n",
    "\n",
    "    # Intersection area\n",
    "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
    "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
    "\n",
    "    # Union Area\n",
    "    union = w1 * h1 + w2 * h2 - inter + eps\n",
    "\n",
    "    # IoU\n",
    "    iou = inter / union\n",
    "    if CIoU or DIoU or GIoU:\n",
    "        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex (smallest enclosing box) width\n",
    "        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n",
    "        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n",
    "            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
    "            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center dist ** 2\n",
    "            if CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "                v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / (h2 + eps)) - torch.atan(w1 / (h1 + eps)), 2)\n",
    "                with torch.no_grad():\n",
    "                    alpha = v / (v - iou + (1 + eps))\n",
    "                return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
    "            return iou - rho2 / c2  # DIoU\n",
    "        c_area = cw * ch + eps  # convex area\n",
    "        return iou - (c_area - union) / c_area  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n",
    "    return iou  # IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e7274ed0f6198f0b80ec6c61f69697cbc6657ab90adeb9c17021c87e139f38f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
