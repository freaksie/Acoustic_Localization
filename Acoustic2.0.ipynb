{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 15:55:13.189322: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-29 15:55:13.342525: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-29 15:55:13.377276: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-29 15:55:13.901866: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/neel/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.8/lib64:\n",
      "2022-12-29 15:55:13.901948: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/neel/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:/usr/local/cuda-11.8/lib64:\n",
      "2022-12-29 15:55:13.901954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from collections import Counter\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.optimizers import Adam, Adamax, SGD\n",
    "import imutils\n",
    "import tensorflow_addons as tfa\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "<tensorflow.python.eager.context._EagerDeviceContext object at 0x7ff6375a7380>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 15:55:19.419126: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.context._EagerDeviceContext at 0x7ff632877fc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 15:55:21.183474: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "2022-12-29 15:55:21.183615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7325 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:19:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "gpu = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "print(gpu)\n",
    "tf.keras.backend.clear_session()\n",
    "TF_ENABLE_ONEDNN_OPTS=0\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(tf.device('/gpu:0'))\n",
    "\n",
    "tf.device('/gpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,sample_rate=librosa.load(\"/home/neel/Acoustic/Acoustics/dataset/datachunks/channel1_normal_train107_23.wav\",sr=None)\n",
    "duration_GT= len(_)/sample_rate\n",
    "label_file_path=\"/home/neel/Acoustic/Acoustics/dataset/labels/\"\n",
    "labels=[]\n",
    "for i in os.listdir(label_file_path):\n",
    "    labels.append(i[:-4])\n",
    "dataset=[]\n",
    "for label in labels:\n",
    "    row=[]\n",
    "    file=open(label_file_path+label+\".txt\",\"r\")\n",
    "    label_s=file.read()\n",
    "    label_list=label_s.replace('\\n','').split(\" \")\n",
    "    channel1=\"channel1_\"+label+\".wav\"\n",
    "    channel2=\"channel2_\"+label+\".wav\"\n",
    "    audio,sample_rate=librosa.load(\"dataset/datachunks/\"+channel1,sr=None)\n",
    "    duration = len(audio)/sample_rate\n",
    "    if duration == duration_GT:\n",
    "        row.append(channel1)\n",
    "        row.append(channel2)\n",
    "        for cord in label_list[1:]:\n",
    "            row.append(cord)\n",
    "        X=float(row[2])\n",
    "        Y=float(row[3])\n",
    "        W=float(row[4])\n",
    "        H=float(row[5])\n",
    "        row[2]=X-(W/2)\n",
    "        row[3]=Y-(H/2)\n",
    "        row[4]=X+(W/2)\n",
    "        row[5]=Y+(H/2)\n",
    "        dataset.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.to_csv('dataset/train.csv', header=False, index=False)\n",
    "del dataset, datasets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"/home/neel/Acoustic/Acoustics/dataset\"\n",
    "audio_path=os.path.sep.join([base_path,\"datachunks\"])\n",
    "annots_path=os.path.sep.join([base_path,\"train.csv\"])\n",
    "\n",
    "base_output=\"/home/neel/Acoustic/Acoustics/output2.0\"\n",
    "model_path=os.path.sep.join([base_output,\"detector.h5\"])\n",
    "plot_path=os.path.sep.join([base_output,\"plot.png\"])\n",
    "test_file=os.path.sep.join([base_output,\"test.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading dataset...\")\n",
    "rows = open(annots_path).read().strip().split(\"\\n\")\n",
    "spectrogram=np.empty(((len(rows)),525,553,2), dtype=\"float32\")\n",
    "bounding_box_cords=[]\n",
    "window_size=int(1048)\n",
    "wd = signal.windows.hamming(window_size)\n",
    "slide_size = int(1)\n",
    "overlap = window_size - slide_size\n",
    "filenames=[]\n",
    "cnt=0\n",
    "for row in rows:\n",
    "    # print(cnt)\n",
    "    row = row.split(\",\")\n",
    "    (filename1,filename2,X, Y, W, H) = row\n",
    "    channel1_Path = os.path.sep.join([audio_path, filename1])\n",
    "    channel2_path = os.path.sep.join([audio_path, filename2])\n",
    "    \n",
    "    channel1,sample_rate=librosa.load(channel1_Path,sr=None)\n",
    "    channel2,sample_rate=librosa.load(channel2_path,sr=None)\n",
    "\n",
    "    # image1 = cv2.imread(imagePath1)\n",
    "    # image2=cv2.imread(imagePath2)\n",
    "    # (h, w) = image1.shape[:2]\n",
    "\n",
    "    # startX = float(startX) / w\n",
    "    # startY = float(startY) / h\n",
    "    # endX = float(endX) / w\n",
    "    # endY = float(endY) / h\n",
    "\n",
    "    frequency,time,spectrum1=signal.spectrogram(channel1,nfft=window_size,fs=sample_rate,window=wd,noverlap=overlap,mode='magnitude')\n",
    "    frequency,time,spectrum2=signal.spectrogram(channel2,nfft=window_size,fs=sample_rate,window=wd,noverlap=overlap,mode='magnitude')\n",
    "\n",
    "    # print(image1.shape)\n",
    "    data=np.stack((spectrum1,spectrum2),axis=-1)\n",
    "    spectrogram[cnt]=data\n",
    "    del data,frequency,time\n",
    "    cnt+=1\n",
    "    bounding_box_cords.append((X, Y, W, H))\n",
    "    filenames.append([filename1,filename2])\n",
    "    # filenames.append(filename1)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array(bounding_box_cords, dtype=\"float32\")\n",
    "\n",
    "del bounding_box_cords,rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8831, 4)\n",
      "(8831, 525, 553, 2)\n"
     ]
    }
   ],
   "source": [
    "print(targets.shape)\n",
    "print(spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving testing filenames...\n"
     ]
    }
   ],
   "source": [
    "split = train_test_split(spectrogram, targets, filenames, test_size=0.10,random_state=42)\n",
    "del spectrogram, targets,filenames\n",
    "(trainData, testData) = split[:2]\n",
    "(trainTargets, testTargets) = split[2:4]\n",
    "(trainFilenames, testFilenames) = split[4:]\n",
    "del split\n",
    "\n",
    "print(\"[INFO] saving testing filenames...\")\n",
    "f = open(test_file, \"w\")\n",
    "for i in testFilenames:\n",
    "    f.write(i[0]+\",\"+i[1])\n",
    "    f.write(\"\\n\")\n",
    "# f.write(\"\\n\".join(testFilenames))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "model= tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32,(3,3), padding='valid',strides=2, input_shape=(525,553,2), activation='relu'))\n",
    "# model.add(LeakyReLU(alpha=0.03))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='valid'))\n",
    "# model.add(tf.keras.layers.Conv2D(32,(3,3), padding='valid',strides=2,activation='relu'))\n",
    "# # model.add(LeakyReLU(alpha=0.03))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='valid'))\n",
    "# model.add(tf.keras.layers.Conv2D(64,(3,3), padding='valid', strides=2,activation='relu'))\n",
    "# # model.add(LeakyReLU(alpha=0.03))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='valid'))\n",
    "# model.add(tf.keras.layers.Conv2D(64,(3,3), padding='valid',strides=2,activation='relu'))\n",
    "# # model.add(LeakyReLU(alpha=0.03))\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='relu'))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128,activation='relu'))\n",
    "model.add(LeakyReLU(alpha=0.03))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(64,activation='relu'))\n",
    "# model.add(LeakyReLU(alpha=0.03))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(32,activation='relu'))\n",
    "# model.add(LeakyReLU(alpha=0.03))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(4,activation='relu'))\n",
    "# model.add(LeakyReLU(alpha=0.03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 262, 276, 32)      608       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 262, 276, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 131, 138, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 578496)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               74047616  \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 128)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 74,059,716\n",
      "Trainable params: 74,059,204\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "init_lr=1e-4\n",
    "\n",
    "epoch=10\n",
    "batch_size=32\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    init_lr,\n",
    "    decay_steps=20,\n",
    "    decay_rate=0.1,\n",
    "    staircase=True)\n",
    "\n",
    "opt = SGD(learning_rate=lr_schedule ,momentum=0.9, )\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='mse',\n",
    "  metrics=['accuracy'],run_eagerly=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 15:59:58.082423: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 18457702200 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 569966592/51049857024\n",
      "2022-12-29 15:59:58.082451: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                      7681802240\n",
      "InUse:                      1194247072\n",
      "MaxInUse:                   1786581024\n",
      "NumAllocs:                         502\n",
      "MaxAllocSize:                296189952\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-12-29 15:59:58.082468: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2022-12-29 15:59:58.082488: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 29\n",
      "2022-12-29 15:59:58.082491: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 9\n",
      "2022-12-29 15:59:58.082494: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16, 4\n",
      "2022-12-29 15:59:58.082496: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 128, 40\n",
      "2022-12-29 15:59:58.082498: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 256, 20\n",
      "2022-12-29 15:59:58.082501: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 512, 24\n",
      "2022-12-29 15:59:58.082503: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1\n",
      "2022-12-29 15:59:58.082505: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2304, 4\n",
      "2022-12-29 15:59:58.082507: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8192, 4\n",
      "2022-12-29 15:59:58.082510: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 32768, 4\n",
      "2022-12-29 15:59:58.082513: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2322600, 4\n",
      "2022-12-29 15:59:58.082515: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 296189952, 4\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31484/1920743125.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \tH = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m                 \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainTargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestTargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "\tH = model.fit(\n",
    "\t\ttrainData, trainTargets,\n",
    "\t\tvalidation_data=(testData, testTargets),\n",
    "\t\tbatch_size=batch_size,\n",
    "\t\tepochs=epoch,\n",
    "\t\tverbose=1)\n",
    "\tprint(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving object detector model...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] saving object detector model...\")\n",
    "model.save(model_path, save_format=\"h5\")\n",
    "N = epoch\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_loss\")\n",
    "# plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_loss\")\n",
    "# plt.title(\"Bounding Box Regression MSE on Training Set\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del spectrum1, spectrum2, trainData, testData, model, trainTargets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step\n",
      "/home/neel/Acoustic/Acoustics/dataset/datachunks/channel1_4m_train51_3.wav\n",
      "Ground truth:   0.556597 0.60463 0.0798611 0.0666667\n",
      "\n",
      "Predicted:  0.0 9.353084e-05 0.0 1.4486088e-05\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "/home/neel/Acoustic/Acoustics/dataset/datachunks/channel1_normal_train119_26.wav\n",
      "Ground truth:   0.711458 0.462037 0.0576389 0.0462963\n",
      "\n",
      "Predicted:  0.0 6.1393635e-05 5.4580255e-07 2.8672457e-05\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "/home/neel/Acoustic/Acoustics/dataset/datachunks/channel1_5m_train29_23.wav\n",
      "Ground truth:   0.100694 0.580093 0.0652778 0.0509259\n",
      "\n",
      "Predicted:  0.0 5.716248e-05 4.8393536e-05 2.7515198e-05\n"
     ]
    }
   ],
   "source": [
    "model=load_model('/home/neel/Acoustic/Acoustics/output2.0/detector.h5')\n",
    "for i,img in enumerate(testFilenames):\n",
    "    imagePath1=os.path.sep.join([audio_path, img[0]])\n",
    "    imagePath2=os.path.sep.join([audio_path, img[1]])\n",
    "    channel1,sample_rate=librosa.load(imagePath1,sr=None)\n",
    "    channel2,sample_rate=librosa.load(imagePath2,sr=None)\n",
    "    \n",
    "    frequency,time,spectrum1=signal.spectrogram(channel1,nfft=window_size,fs=sample_rate,window=wd,noverlap=overlap,mode='magnitude')\n",
    "    frequency,time,spectrum2=signal.spectrogram(channel2,nfft=window_size,fs=sample_rate,window=wd,noverlap=overlap,mode='magnitude')\n",
    "    spectrum1=np.expand_dims(spectrum1,axis=0)\n",
    "    spectrum2=np.expand_dims(spectrum2,axis=0)\n",
    "    image=np.stack((spectrum1,spectrum2),axis=-1)\n",
    "\n",
    "    preds = model.predict(image)[0]\n",
    "    (startX, startY, endX, endY) = preds\n",
    "    print(imagePath1)\n",
    "    # image2 = cv2.imread(os.path.sep.join([\"/home/neel/Acoustic/yolov5_training/img_data/frames\", img[0][9:]]))\n",
    "    # image2 = imutils.resize(image2, width=600)\n",
    "    # (h, w) = image1.shape[:2]\n",
    "    # startX = startX*w\n",
    "    # startY = startY*h\n",
    "    # endX = endX*w\n",
    "    # endY = endY*h\n",
    "    f=open(\"/home/neel/Acoustic/Acoustics/dataset/labels/\"+img[0][9:-4]+\".txt\")\n",
    "    ground_truth=f.read()\n",
    "    print(\"Ground truth: \",ground_truth[1:])\n",
    "    print(\"Predicted: \",startX, startY, endX, endY)\n",
    "    # cv2.rectangle(image2, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "\t# # show the output image\n",
    "    # cv2.imwrite(\"/home/neel/Acoustic/Acoustics/output/test_img/\"+img[0], image2)\n",
    "    # cv2.imshow(\"Output\", image2)\n",
    "    # cv2.waitKey(0)\n",
    "    if i==2:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 15:59:07.416994: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "0.0 0.16537079005502164 0.05323603400029242 -0.09584552783053368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 15:59:08.725378: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/neel/Acoustic/yolov5_training/img_data/frames/3m_train2_5.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31484/2561942900.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstartY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/neel/Acoustic/yolov5_training/img_data/frames/3m_train2_5.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstartY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2158\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1558\u001b[0m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1560\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimg_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1561\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[1;32m   1562\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3091\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3092\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3093\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/neel/Acoustic/yolov5_training/img_data/frames/3m_train2_5.jpg'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from matplotlib import patches\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "fig = plt.figure()\n",
    "channel1,sample_rate=librosa.load(\"/home/neel/Acoustic/Acoustics/dataset/datachunks/channel1_3m_train2_5.wav\",sr=None)\n",
    "channel2,sample_rate=librosa.load(\"/home/neel/Acoustic/Acoustics/dataset/datachunks/channel2_3m_train2_5.wav\",sr=None)\n",
    "frequency,time,spectrum1=signal.spectrogram(channel1,nfft=window_size,fs=sample_rate,window=wd,noverlap=overlap,mode='magnitude')\n",
    "frequency,time,spectrum2=signal.spectrogram(channel2,nfft=window_size,fs=sample_rate,window=wd,noverlap=overlap,mode='magnitude')\n",
    "spectrum1=np.expand_dims(spectrum1,axis=0)\n",
    "spectrum2=np.expand_dims(spectrum2,axis=0)\n",
    "image=np.stack((spectrum1,spectrum2),axis=-1)\n",
    "preds = model.predict(image)[0]\n",
    "(startX, startY, endX, endY) = preds\n",
    "startX,startY,endX, endY=startX*1440,startY*1080, endX*1440, endY*1080\n",
    "width=endX-startX\n",
    "height=endY-startY\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "print(startX,startY,width,height)\n",
    "image = plt.imread('/home/neel/Acoustic/yolov5_training/img_data/frames/3m_train2_5.jpg')\n",
    "plt.imshow(image)\n",
    "rect = patches.Rectangle((startX,startY), width, height, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e7274ed0f6198f0b80ec6c61f69697cbc6657ab90adeb9c17021c87e139f38f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
